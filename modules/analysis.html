<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<meta name="viewport" content="width=device-width">
<link rel="stylesheet" href="/ashe_bayes/assets/css/style.css">
<link rel="stylesheet" href="/ashe_bayes/assets/css/syntax_default.css">
<!-- <link rel="shortcut icon" type="image/png" href="/ashe_bayes/assets/img/favicon.ico"> -->
<link crossorigin="anonymous" media="all" integrity="sha512-uhAd27cNiLn0VE2GVEVUN8D5zW0o7s0QTnCGMnJZkL2HqN9/LwHDi4ndTPJH0upUQHYl/8QF6cwbOYp/KIzlJQ==" rel="stylesheet" href="https://github.githubassets.com/assets/github-be4e45349cf088df7a6636f437c0a167.css" />
<script defer src="/ashe_bayes/assets/js/all.min.js"></script>
<!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>
<script async defer src="https://buttons.github.io/buttons.js"></script>
<script src="https://code.jquery.com/jquery-3.2.1.min.js"></script>

<title>A Gentle Introduction to Bayesian Analysis with Applications to QuantCrit</title>

  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>A Gentle Introduction to Bayesian Analysis with Applications to QuantCrit<a href="https://github.com/btskinner/ashe_bayes" class="iconlink">
	      <i class="fab fa-github fa-sm"></i></a></h1>
	<h2 class="thin"></h2>
	<p>A workshop to introduce quantitatively-trained researchers to the Bayesian
paradigm with applications to QuantCrit
</p>
	<!-- side / top bar menu -->
	<h2 class="thin">
	  <a href="/ashe_bayes/">Welcome</a></br>
    <a href="/ashe_bayes/setup/">Setup</a></br>
    <a href="/ashe_bayes/modules/">Modules</a></br>
    <a href="/ashe_bayes/resources/">Resources</a></br>
	  <a href="/ashe_bayes/about/">About</a></br>
	</h2>
      </header>

      <section>
	<h1>Analytic Examples</h1>
	

	
<p>
  
  <a href="/ashe_bayes/assets/pdf/analysis.pdf"
     class="iconlink" download title="Get PDF of modules">
  <i class="far fa-file-pdf fa-2x"></i>
  </a>
  
  &nbsp;&nbsp;
  
  <a href="/ashe_bayes/scripts/analysis.R"
     class="iconlink" download title="Get script">
    <i class="fas fa-code fa-2x"></i>
  </a>
  &nbsp;&nbsp;
  
  
  
  
  
  <a href="/ashe_bayes/data/college.RDS"
     class="iconlink" download title="Get data">
    <i class="fas fa-database fa-2x"></i>
  </a>
  &nbsp;&nbsp;
  
  
</p>


<p>In this part of the workshop, we’ll work through a couple of examples of a
Bayesian analysis. The first will be a very simple regression and the second a
more complex regression with deep interactions. We’ll also consider a few model
checks and ways of presenting the results.</p>

<h2 id="libraries">Libraries</h2>

<p>We’ll load a few libraries for our analysis:</p>

<ul>
  <li><em>tidyverse</em>: useful for data wrangling</li>
  <li><em>brms</em>: our main Bayesian regression tool</li>
  <li><em>bayesplot</em>: support library for plotting Bayesian results</li>
  <li><em>tidybayes</em>: support library for wrangling/plotting Bayesian results</li>
  <li><em>patchwork</em>: combine plots</li>
  <li><em>shinystan</em>: interactive inspection of Bayesian objects</li>
  <li><em>parallel</em>: take advantage of multiple cores</li>
</ul>

<p>If you don’t have these libraries, you can install them quickly with the
following:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">install.packages</span><span class="p">(</span><span class="nf">c</span><span class="p">(</span><span class="s2">"tidyverse"</span><span class="p">,</span><span class="w"> </span><span class="s2">"brms"</span><span class="p">,</span><span class="w"> </span><span class="s2">"bayesplot"</span><span class="p">,</span><span class="w"> </span><span class="s2">"tidybayes"</span><span class="p">,</span><span class="w"> </span><span class="s2">"patchwork"</span><span class="p">,</span><span class="w"> </span><span class="s2">"shinystan"</span><span class="p">,</span><span class="w"> </span><span class="s2">"parallel"</span><span class="p">),</span><span class="w"> </span><span class="n">dependencies</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">

</span></code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## ---------------------------</span><span class="w">
</span><span class="c1">## libraries</span><span class="w">
</span><span class="c1">## ---------------------------</span><span class="w">

</span><span class="n">libs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"tidyverse"</span><span class="p">,</span><span class="w"> </span><span class="s2">"brms"</span><span class="p">,</span><span class="w"> </span><span class="s2">"bayesplot"</span><span class="p">,</span><span class="w"> </span><span class="s2">"tidybayes"</span><span class="p">,</span><span class="w"> </span><span class="s2">"patchwork"</span><span class="p">,</span><span class="s2">"shinystan"</span><span class="p">)</span><span class="w">
</span><span class="n">sapply</span><span class="p">(</span><span class="n">libs</span><span class="p">,</span><span class="w"> </span><span class="n">require</span><span class="p">,</span><span class="w"> </span><span class="n">character.only</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Loading required package: tidyverse
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
## ✔ dplyr     1.1.3     ✔ readr     2.1.4
## ✔ forcats   1.0.0     ✔ stringr   1.5.0
## ✔ ggplot2   3.4.4     ✔ tibble    3.2.1
## ✔ lubridate 1.9.3     ✔ tidyr     1.3.0
## ✔ purrr     1.0.2     
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()
## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors
## Loading required package: brms
## 
## Loading required package: Rcpp
## 
## Loading 'brms' package (version 2.20.4). Useful instructions
## can be found by typing help('brms'). A more detailed introduction
## to the package is available through vignette('brms_overview').
## 
## 
## Attaching package: 'brms'
## 
## 
## The following object is masked from 'package:stats':
## 
##     ar
## 
## 
## Loading required package: bayesplot
## 
## This is bayesplot version 1.10.0
## 
## - Online documentation and vignettes at mc-stan.org/bayesplot
## 
## - bayesplot theme set to bayesplot::theme_default()
## 
##    * Does _not_ affect other ggplot2 plots
## 
##    * See ?bayesplot_theme_set for details on theme setting
## 
## 
## Attaching package: 'bayesplot'
## 
## 
## The following object is masked from 'package:brms':
## 
##     rhat
## 
## 
## Loading required package: tidybayes
## 
## 
## Attaching package: 'tidybayes'
## 
## 
## The following objects are masked from 'package:brms':
## 
##     dstudent_t, pstudent_t, qstudent_t, rstudent_t
## 
## 
## Loading required package: patchwork
## 
## Loading required package: shinystan
## 
## Loading required package: shiny
## 
## 
## This is shinystan version 2.6.0
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## tidyverse      brms bayesplot tidybayes patchwork shinystan 
##      TRUE      TRUE      TRUE      TRUE      TRUE      TRUE
</code></pre></div></div>

<h2 id="settings">Settings</h2>

<p>We have a couple of settings that will help us. First, we’ll take advantage of
our computers’ multiple cores with <code class="language-plaintext highlighter-rouge">options(mc.cores=parallel::detectCores())</code>.</p>

<p>Because the estimation of Bayesian models involve pseudorandom computational
processes, we’ll set a seed so that we get the same results.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## ---------------------------</span><span class="w">
</span><span class="c1">## settings</span><span class="w">
</span><span class="c1">## ---------------------------</span><span class="w">

</span><span class="c1">## set number of cores to use to speed things up</span><span class="w">
</span><span class="n">options</span><span class="p">(</span><span class="n">mc.cores</span><span class="o">=</span><span class="n">parallel</span><span class="o">::</span><span class="n">detectCores</span><span class="p">())</span><span class="w">

</span><span class="c1">## set a seed so things stay the same</span><span class="w">
</span><span class="n">my_seed</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">20231118</span><span class="w">
</span></code></pre></div></div>

<h2 id="data">Data</h2>

<p>The data we’re using today is a simplified version of HSLS09. Observations with
missing data have been dropped. We’re also not accounting for the weighting
scheme used by NCES.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## ---------------------------</span><span class="w">
</span><span class="c1">## input</span><span class="w">
</span><span class="c1">## ---------------------------</span><span class="w">

</span><span class="n">df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">readRDS</span><span class="p">(</span><span class="s2">"college.RDS"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## ---------------------------</span><span class="w">
</span><span class="c1">## show data set</span><span class="w">
</span><span class="c1">## ---------------------------</span><span class="w">

</span><span class="n">df</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## # A tibble: 17,202 × 7
##       id gender raceeth birthyr pov185 region    college
##    &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;       &lt;dbl&gt;
##  1 10001 male   white      1995 above  midwest         1
##  2 10002 female white      1995 below  northeast       1
##  3 10003 female blackaa    1995 above  west            1
##  4 10004 female white      1995 above  south           0
##  5 10005 male   white      1995 above  south           0
##  6 10007 female white      1994 above  northeast       1
##  7 10008 male   white      1994 above  northeast       1
##  8 10009 male   white      1995 above  south           1
##  9 10012 female white      1995 above  midwest         1
## 10 10013 male   white      1995 above  south           1
## # ℹ 17,192 more rows
</code></pre></div></div>

<h2 id="simple-regression">Simple regression</h2>

<p>As a first step, we’ll fit a regression of college attendance on a vector of
ones (intercept only model). This will give us the average attendance across the
full sample, with measure of uncertainty in the spread of the posterior
distribution.</p>

<p>With Bayesian models, we have to be a little more particular with our
distributional assumptions. Since our outcomes fall into 0s and 1s, we should
use a <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli
distribution</a> with a logit
link function for our likelihood in the <code class="language-plaintext highlighter-rouge">family</code> argument:</p>

\[f(k;p) = p^k(1-p)^{1-k}\, \text{for}\, k \in {0,1}\]

<p>We know our <em>k</em> values here (0 for no college attendance and 1 for attendance).
Our unknown is <em>p</em>, the probability of attendance.</p>

<h3 id="running-the-model">Running the model</h3>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## -----------------------------------------------------------------------------</span><span class="w">
</span><span class="c1">## simple regression: intercept only (average college-going rate)</span><span class="w">
</span><span class="c1">## -----------------------------------------------------------------------------</span><span class="w">

</span><span class="c1">## likelihood of going to college</span><span class="w">
</span><span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">brm</span><span class="p">(</span><span class="n">college</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w">
           </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="p">,</span><span class="w">
           </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bernoulli</span><span class="p">(</span><span class="s2">"logit"</span><span class="p">),</span><span class="w">
           </span><span class="n">seed</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">my_seed</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>You should notice a few things. First, your model has to be compiled to a faster
coding language. For short models, it can take longer to compile than to run,
but for longer models, this isn’t the case. You can also save the models so that
you don’t have recompile each time.</p>

<p>Once it starts to run, you’ll see a lot of information about leapfrog steps,
chains, warm-ups, and samples. We’ll discuss some of these pieces and what they
mean in just a bit, but first we’ll take a look at the summary statistics.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## show summary stats</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>##  Family: bernoulli 
##   Links: mu = logit 
## Formula: college ~ 1 
##    Data: df (Number of observations: 17202) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.12      0.02     1.08     1.15 1.00     1534     1792
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
</code></pre></div></div>

<p>The intercept value is approximately 1.12. Remembering that this is on the logit
scale, we can covert this using an inverse logit transformation:</p>

<p><strong>Logit</strong></p>

\[ln(\frac{p}{1-p})\]

<p><strong>Inverse logit</strong></p>

\[\frac{1}{1 + e^{-x}}\]

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## helper function: inverse logit</span><span class="w">
</span><span class="n">inv_logit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span><span class="w"> </span><span class="p">}</span><span class="w">

</span><span class="c1">## convert intercept value to probability scale</span><span class="w">
</span><span class="n">inv_logit</span><span class="p">(</span><span class="m">1.12</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] 0.7539887
</code></pre></div></div>

<p>Approximately 75% of the sample attended college at some point. We also have an
estimated error of approximately 0.02 (back on the log scale). So far, this
seems much like you may be used seeing after fitting frequentist models. But
what is <code class="language-plaintext highlighter-rouge">Rhat</code>, <code class="language-plaintext highlighter-rouge">Bulk_ESS</code>, and <code class="language-plaintext highlighter-rouge">Tail_ESS</code>? Further, what is the meaning of all
the information in the <code class="language-plaintext highlighter-rouge">Draws</code>?</p>

<h3 id="a-quick-note-on-how-we-actually-estimate-the-posterior">A quick note on how we actually estimate the posterior</h3>

<p>In applied Bayesian work, the posterior distribution, which is proportional to
the prior multiplied by the likelihood distributions, often doesn’t have a
<strong>closed form solution</strong>. This just means we can’t solve for the posterior using
analytic methods (e.g., algebraic equations). Instead, we are forced to sample
from the posterior and build an empirical distribution of our best guesses. This
is easier said than done because, remember, we don’t know the answer.</p>

<p>Imagine I put you in a dark room and told you that I wanted you to be able to
describe the space to me. I assume the floor generally slopes to a low point, so
you’ll probably make your way there. That said, the room may have dips in the
floor or weird corners. It also may be the size of a typical classroom or the
size of a football field. And also, I don’t have infinite time or money for your
to spend your time figuring it out. How do you explore the space in way that’s
complete but also efficient?</p>

<p>This is basically the problem facing applied Bayesian work. With the advent of
modern computing, we have a number of approaches. We unfortunately don’t have
time to get into the nuances, benefits, and drawbacks of various samplers. The
main thing to know is that most make use of Markov chain Monte Carlo (MCMC)
chains and, as a result, return multiple “guesses” of our unknown parameters.
When we see the <code class="language-plaintext highlighter-rouge">Estimate</code> and <code class="language-plaintext highlighter-rouge">Est.Error</code> values in the results, these are
summary statistics of a full distribution of results.</p>

<p><em>Why multiple chains?</em>: Let’s go back to the room. If you only have so much time
to explore the room, where you start may influence where you explore. You may
not find the low point in the floor or you may get stuck in a corner. How do I
know you are giving me an accurate account of the room? But if you can start
multiple times in multiple places and end with similar results, that feels like
evidence you are describing the space, or at least the main features. This is
the logic behind running multiple chains.</p>

<p>Let’s look at beginning of the chains with a <strong>trace plot</strong>, the first 50
samples:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## show trace of chains for intercept (our main parameter)</span><span class="w">
</span><span class="n">color_scheme_set</span><span class="p">(</span><span class="s2">"mix-blue-pink"</span><span class="p">)</span><span class="w">
</span><span class="n">mcmc_trace</span><span class="p">(</span><span class="n">fit</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">as_draws</span><span class="p">(</span><span class="n">inc_warmup</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">),</span><span class="w">
           </span><span class="n">pars</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"b_Intercept"</span><span class="p">,</span><span class="w"> </span><span class="n">n_warmup</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1000</span><span class="p">,</span><span class="w">
           </span><span class="n">window</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">50</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="w">
    </span><span class="n">title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Trace of posterior chains"</span><span class="p">,</span><span class="w">
    </span><span class="n">subtitle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Draws: 0 to 50"</span><span class="w">
  </span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="../figures/trace_simple_reg_1-1.png" width="100%" /></p>

<p>Notice how the four chains start in very different places. But very quickly for
this simple model they collectively <strong>converge</strong> to a similar area. Let’s look
at the rest of the samples:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## show trace of chains for intercept (our main parameter)</span><span class="w">
</span><span class="n">color_scheme_set</span><span class="p">(</span><span class="s2">"mix-blue-pink"</span><span class="p">)</span><span class="w">
</span><span class="n">mcmc_trace</span><span class="p">(</span><span class="n">fit</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">as_draws</span><span class="p">(</span><span class="n">inc_warmup</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">),</span><span class="w">
           </span><span class="n">pars</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"b_Intercept"</span><span class="p">,</span><span class="w"> </span><span class="n">n_warmup</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1000</span><span class="p">,</span><span class="w">
           </span><span class="n">window</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">500</span><span class="p">,</span><span class="w"> </span><span class="m">2000</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="w">
    </span><span class="n">title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Trace of posterior chains"</span><span class="p">,</span><span class="w">
    </span><span class="n">subtitle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Draws: 500 to 2000"</span><span class="w">
  </span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="../figures/trace_simple_reg_2-1.png" width="100%" /></p>

<p>Because we purposefully start the chains in different spots, we shouldn’t think
those early samples represent the posterior. We throw these out as <strong>warmup</strong>
and only keep the later samples which are combined into our <strong>total post-warmup
draws</strong>. In this model, we end up with 4,000 sample draws of our posterior.</p>

<p>Seeing the consistent overlap or <strong>mixing</strong> of the chains, we feel we have a
well-performing model (at least in terms of computation). The <code class="language-plaintext highlighter-rouge">Rhat</code> statistic
gives us a more formal test of the ratio of between chain variance to within
chain variance. <code class="language-plaintext highlighter-rouge">Rhat</code> measures below 1.1 suggest we have good mixture. The two
ESS measures stand for <strong>Effective Sample Size</strong>. Due to the deterministic
nature of our sampler (and most samplers), the draws in our sample distribution
aren’t entirely independent. <code class="language-plaintext highlighter-rouge">Bulk_ESS</code> sample size gives us the overall
effective sample size while <code class="language-plaintext highlighter-rouge">Tail_ESS</code> gives an estimate of effective sample
size in the tails of the distribution.</p>

<h3 id="plotting-the-posterior">Plotting the posterior</h3>

<p>Since our Bayesian posterior represents a distribution of samples, let’s look at
it graphically.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## show distribution of intercept (our main parameter)</span><span class="w">
</span><span class="n">mcmc_areas</span><span class="p">(</span><span class="n">fit</span><span class="p">,</span><span class="w"> </span><span class="n">prob</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.95</span><span class="p">,</span><span class="w"> </span><span class="n">pars</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"b_Intercept"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="w">
    </span><span class="n">title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Posterior distribution (log scale)"</span><span class="p">,</span><span class="w">
    </span><span class="n">subtitle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"with median and 95% interval"</span><span class="w">
  </span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="../figures/plot_simple_reg_logit-1.png" width="100%" /></p>

<p>As a quick trick, we’ll use the <code class="language-plaintext highlighter-rouge">transformation</code> argument and our <code class="language-plaintext highlighter-rouge">inv_logit()</code>
function to see the posterior on the probability scale.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## show distribution of transformed intercept (our main parameter)</span><span class="w">
</span><span class="c1">## using helper function</span><span class="w">
</span><span class="n">mcmc_areas</span><span class="p">(</span><span class="n">fit</span><span class="p">,</span><span class="w"> </span><span class="n">prob</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.95</span><span class="p">,</span><span class="w"> </span><span class="n">pars</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"b_Intercept"</span><span class="p">,</span><span class="w">
           </span><span class="n">transformation</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="s2">"b_Intercept"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inv_logit</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="w">
    </span><span class="n">title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Posterior distribution (probability)"</span><span class="p">,</span><span class="w">
    </span><span class="n">subtitle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"with median and 95% interval; prior: student_t(0,3,2.5)"</span><span class="w">
  </span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="../figures/plot_simple_reg_prob-1.png" width="100%" /></p>

<h3 id="priors">Priors</h3>

<p>What about priors? Don’t we have to set those? By default, <code class="language-plaintext highlighter-rouge">brm()</code> will choose a
set of <strong>weakly informative</strong> priors based on the likelihood distribution you
set. You can check this after the fact using <code class="language-plaintext highlighter-rouge">prior_summary()</code>.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## ---------------------------</span><span class="w">
</span><span class="c1">## check prior</span><span class="w">
</span><span class="c1">## ---------------------------</span><span class="w">

</span><span class="c1">## show prior from first model</span><span class="w">
</span><span class="n">prior_summary</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Intercept ~ student_t(3, 0, 2.5)
</code></pre></div></div>

<p>Let’s change our prior to normal distribution with a wide variance (remember:
we’re on a logit scale).</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## change to normal prior for comparison</span><span class="w">
</span><span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">brm</span><span class="p">(</span><span class="n">college</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w">
           </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="p">,</span><span class="w">
           </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bernoulli</span><span class="p">(</span><span class="s2">"logit"</span><span class="p">),</span><span class="w">
           </span><span class="n">seed</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">my_seed</span><span class="p">,</span><span class="w">
           </span><span class="n">prior</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set_prior</span><span class="p">(</span><span class="s2">"normal(0,20)"</span><span class="p">,</span><span class="w"> </span><span class="n">class</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Intercept"</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Compiling Stan program...
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Start sampling
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## check prior</span><span class="w">
</span><span class="n">prior_summary</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Intercept ~ normal(0,20)
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## show summary stats</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>##  Family: bernoulli 
##   Links: mu = logit 
## Formula: college ~ 1 
##    Data: df (Number of observations: 17202) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.12      0.02     1.08     1.15 1.00     1498     2192
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## show distribution of transformed intercept (our main parameter)</span><span class="w">
</span><span class="c1">## using helper function</span><span class="w">
</span><span class="n">mcmc_areas</span><span class="p">(</span><span class="n">fit</span><span class="p">,</span><span class="w"> </span><span class="n">prob</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.95</span><span class="p">,</span><span class="w"> </span><span class="n">pars</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"b_Intercept"</span><span class="p">,</span><span class="w">
           </span><span class="n">transformation</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="s2">"b_Intercept"</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inv_logit</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="w">
    </span><span class="n">title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Posterior distribution (probability)"</span><span class="p">,</span><span class="w">
    </span><span class="n">subtitle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"with median and 95% interval; prior: normal(0,20)"</span><span class="p">,</span><span class="w">
  </span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="../figures/change_prior-1.png" width="100%" /></p>

<p>The posterior is largely the same. It may have taken slightly longer or shorter
to run the sampler, but again, for this model it’s almost imperceptible. For
more complex models, however, prior selection can be the difference between a
model that converges and one that doesn’t. You can also be more specific with
your priors. But for now, we’ll keep using the defaults.</p>

<h3 id="posterior-check">Posterior check</h3>

<p>Bayesians often talk about accurately modeling the <strong>data generating process</strong>.
You can think about your model being a machine that, if accurately built and
tuned, can be reversed to make predictions (new data) that look like what you
observed. If posterior predictions look quite different from what you observed
in your data, that’s evidence you don’t have a good model of the world that
produced your data.</p>

<p>We can check this by producing a number of predictions from our fitted model
using <code class="language-plaintext highlighter-rouge">posterior_epred()</code> and comparing it to our empirical mean of college
enrollment.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## ---------------------------</span><span class="w">
</span><span class="c1">## posterior predictive check</span><span class="w">
</span><span class="c1">## ---------------------------</span><span class="w">

</span><span class="c1">## plot our posterior predictive values against the college-going rate</span><span class="w">
</span><span class="c1">## that is observed in the data</span><span class="w">
</span><span class="n">ppc_stat</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">pull</span><span class="p">(</span><span class="n">college</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="nf">c</span><span class="p">(),</span><span class="w">
         </span><span class="n">yrep</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">posterior_epred</span><span class="p">(</span><span class="n">fit</span><span class="p">),</span><span class="w">
         </span><span class="n">stat</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mean</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
</code></pre></div></div>

<p><img src="../figures/pp_check-1.png" width="100%" /></p>

<p>Seeing that the distribution of our predictions $y^{rep}$ splits $y$, we have
evidence that our machine/model is well specified.</p>

<h2 id="speed-up-trick">Speed up trick</h2>

<p>An unfortunate thing about Bayesian samplers is that they don’t scale well with
data size. In some cases — such as our own — we can exploit connections
between distributions and sufficient statistics to make our data matrix smaller
and therefore sampler faster.</p>

<p>A Bernoulli distribution (coin flip) can be thought of a binomial distribution
(successes out of trials) in which there is only one trial. By collapsing our
data to a single row of the number of students who went to college out of the
total, we can rewrite our model and speed it up.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## -----------------------------------------------------------------------------</span><span class="w">
</span><span class="c1">## speed up trick</span><span class="w">
</span><span class="c1">## -----------------------------------------------------------------------------</span><span class="w">

</span><span class="c1">## since we're using categorical variables (rather than continuous variables),</span><span class="w">
</span><span class="c1">## collapse binary data (bernoulli) into smaller data set of successes/trials</span><span class="w">
</span><span class="c1">## (binomial) to take advantage of sufficient statistics</span><span class="w">
</span><span class="n">df_tmp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">df</span><span class="w"> </span><span class="o">|&gt;</span><span class="w">
  </span><span class="n">summarise</span><span class="p">(</span><span class="n">college</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">college</span><span class="p">),</span><span class="w">
            </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n</span><span class="p">())</span><span class="w">

</span><span class="c1">## likelihood of going to college using binomial</span><span class="w">
</span><span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">brm</span><span class="p">(</span><span class="n">college</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">trials</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w">
           </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df_tmp</span><span class="p">,</span><span class="w">
           </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">binomial</span><span class="p">(</span><span class="s2">"logit"</span><span class="p">),</span><span class="w">
           </span><span class="n">seed</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">my_seed</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Compiling Stan program...
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Start sampling
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## show summary stats</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>##  Family: binomial 
##   Links: mu = logit 
## Formula: college | trials(n) ~ 1 
##    Data: df_tmp (Number of observations: 1) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.12      0.02     1.08     1.15 1.00     1533     2023
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
</code></pre></div></div>

<p>As we can see, we get the same results. We’ll use this speed up trick with our
more complex model below.</p>

<h2 id="multiple-regression-model-with-deep-interactions">Multiple regression model with deep interactions</h2>

<p>In this model, we’ll interact our categorical indicators for race/ethnicity,
gender, and poverty status. We’ll include main effects for each characteristic
individually, but also random intercept adjustments for interactions for each
group. These random effects will be especially important for providing estimates
for small groups.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## -----------------------------------------------------------------------------</span><span class="w">
</span><span class="c1">## multiple regression across groups</span><span class="w">
</span><span class="c1">## -----------------------------------------------------------------------------</span><span class="w">

</span><span class="c1">## collapse into groups of race/ethnicity by gender by poverty level</span><span class="w">
</span><span class="n">df_tmp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">df</span><span class="w"> </span><span class="o">|&gt;</span><span class="w">
  </span><span class="n">group_by</span><span class="p">(</span><span class="n">raceeth</span><span class="p">,</span><span class="w"> </span><span class="n">gender</span><span class="p">,</span><span class="w"> </span><span class="n">pov185</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w">
  </span><span class="n">summarise</span><span class="p">(</span><span class="n">college</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">college</span><span class="p">),</span><span class="w">
            </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n</span><span class="p">(),</span><span class="w">
            </span><span class="n">.groups</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"drop"</span><span class="p">)</span><span class="w">

</span><span class="c1">## likelihood of going to college using binomial</span><span class="w">
</span><span class="n">fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">brm</span><span class="p">(</span><span class="n">college</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">trials</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">raceeth</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">gender</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">pov185</span><span class="w"> </span><span class="o">+</span><span class="w">
             </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">raceeth</span><span class="o">:</span><span class="n">gender</span><span class="o">:</span><span class="n">pov185</span><span class="p">),</span><span class="w">
           </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df_tmp</span><span class="p">,</span><span class="w">
           </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">binomial</span><span class="p">(</span><span class="s2">"logit"</span><span class="p">),</span><span class="w">
           </span><span class="n">seed</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">my_seed</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Compiling Stan program...
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Start sampling
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## show summary stats</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>##  Family: binomial 
##   Links: mu = logit 
## Formula: college | trials(n) ~ raceeth + gender + pov185 + (1 | raceeth:gender:pov185) 
##    Data: df_tmp (Number of observations: 52) 
##   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup draws = 4000
## 
## Group-Level Effects: 
## ~raceeth:gender:pov185 (Number of levels: 52) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.11      0.04     0.04     0.18 1.00      984      649
## 
## Population-Level Effects: 
##                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept          1.46      0.21     1.03     1.89 1.01      717     1389
## raceethasian       1.75      0.23     1.30     2.21 1.00      785     1665
## raceethblackaa     0.47      0.22     0.05     0.91 1.00      769     1481
## raceethhispnr      0.40      0.25    -0.09     0.91 1.00      913     1866
## raceethhisprs      0.48      0.22     0.05     0.91 1.01      697     1508
## raceethmoretor     0.59      0.22     0.15     1.03 1.00      759     1412
## raceethnhpi        0.27      0.35    -0.41     0.99 1.00     1379     2329
## raceethunknown     1.00      0.25     0.52     1.48 1.00      904     1684
## raceethwhite       0.50      0.22     0.07     0.93 1.01      702     1532
## gendermale        -0.48      0.06    -0.59    -0.37 1.00     2653     2620
## pov185below       -1.21      0.08    -1.35    -1.06 1.00     1992     2762
## pov185unknown     -1.30      0.08    -1.44    -1.14 1.00     1869     2048
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
</code></pre></div></div>

<h3 id="shiny-stan">Shiny Stan</h3>

<p>A wonderful tool for inspecting our model fit is Shiny Stan. It performs many of
the checks we performed before plus many more in an interactive browser window.
To open the program, type the following on the console line:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">launch_shinystan</span><span class="p">(</span><span class="n">fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<h3 id="predictions">Predictions</h3>

<p>To make these marginal effect estimates easier to interpret, we’ll produce
posterior predictions again. One thing that’s very easy to do with a model like
this is make predictions even for groups that aren’t observed in the data. To do
that, we will make a design matrix that includes all possible combinations of
the demographic characteristics we modeled.</p>

<p>After that, we’ll produce a plot showing the posterior predictions for each
group.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## ---------------------------</span><span class="w">
</span><span class="c1">## posterior predictions</span><span class="w">
</span><span class="c1">## ---------------------------</span><span class="w">

</span><span class="c1">## create a design matrix (data frame) of all possible groups in our model</span><span class="w">
</span><span class="n">df_design</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">expand.grid</span><span class="p">(</span><span class="n">raceeth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">distinct</span><span class="p">(</span><span class="n">raceeth</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">pull</span><span class="p">()</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="nf">c</span><span class="p">(),</span><span class="w">
                         </span><span class="n">gender</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">distinct</span><span class="p">(</span><span class="n">gender</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">pull</span><span class="p">()</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="nf">c</span><span class="p">(),</span><span class="w">
                         </span><span class="n">pov185</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">distinct</span><span class="p">(</span><span class="n">pov185</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">pull</span><span class="p">()</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="nf">c</span><span class="p">(),</span><span class="w">
                         </span><span class="n">stringsAsFactors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w">
  </span><span class="n">as_tibble</span><span class="p">()</span><span class="w"> </span><span class="o">|&gt;</span><span class="w">
  </span><span class="n">arrange</span><span class="p">(</span><span class="n">raceeth</span><span class="p">,</span><span class="w"> </span><span class="n">gender</span><span class="p">,</span><span class="w"> </span><span class="n">pov185</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w">
  </span><span class="n">mutate</span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w">
         </span><span class="n">group</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">paste</span><span class="p">(</span><span class="n">raceeth</span><span class="p">,</span><span class="w"> </span><span class="n">gender</span><span class="p">,</span><span class="w"> </span><span class="n">pov185</span><span class="p">,</span><span class="w"> </span><span class="n">sep</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"_"</span><span class="p">))</span><span class="w">

</span><span class="c1">## get posterior predictions but in long form that's better for plotting</span><span class="w">
</span><span class="n">pp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">df_design</span><span class="w"> </span><span class="o">|&gt;</span><span class="w">
  </span><span class="n">add_epred_draws</span><span class="p">(</span><span class="n">fit</span><span class="p">,</span><span class="w">
                  </span><span class="n">ndraws</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">500</span><span class="p">,</span><span class="w">
                  </span><span class="n">allow_new_levels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">

</span><span class="c1">## compute mean posterior by group to get order for plot</span><span class="w">
</span><span class="n">pp_mean</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">pp</span><span class="w"> </span><span class="o">|&gt;</span><span class="w">
  </span><span class="n">summarise</span><span class="p">(</span><span class="n">pp_mean</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mean</span><span class="p">(</span><span class="n">.epred</span><span class="p">),</span><span class="w">
            </span><span class="n">.groups</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"drop"</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w">
  </span><span class="n">arrange</span><span class="p">(</span><span class="n">pp_mean</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w">
  </span><span class="n">mutate</span><span class="p">(</span><span class="n">plot_index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">row_number</span><span class="p">(),</span><span class="w">
         </span><span class="n">plot_index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">factor</span><span class="p">(</span><span class="n">plot_index</span><span class="p">,</span><span class="w">
                             </span><span class="n">levels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">plot_index</span><span class="p">,</span><span class="w">
                             </span><span class="n">labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">group</span><span class="p">))</span><span class="w"> </span><span class="o">|&gt;</span><span class="w">
  </span><span class="n">select</span><span class="p">(</span><span class="n">group</span><span class="p">,</span><span class="w"> </span><span class="n">pp_mean</span><span class="p">,</span><span class="w"> </span><span class="n">plot_index</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## join means back to main pp tibble and plot densities for each group</span><span class="w">
</span><span class="n">bayes_g</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">pp</span><span class="w"> </span><span class="o">|&gt;</span><span class="w">
  </span><span class="n">left_join</span><span class="p">(</span><span class="n">pp_mean</span><span class="p">,</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"group"</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w">
  </span><span class="n">ggplot</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">plot_index</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">.epred</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">stat_pointinterval</span><span class="p">(</span><span class="n">.width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.95</span><span class="p">,</span><span class="w"> </span><span class="n">linewidth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.7</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_x_continuous</span><span class="p">(</span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="p">),</span><span class="w">
                     </span><span class="n">minor_breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="w">
    </span><span class="n">title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Posterior predictive distributions of college enrollment"</span><span class="p">,</span><span class="w">
    </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Group: race/ethnicity + gender + poverty status (185%)"</span><span class="p">,</span><span class="w">
    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Rate of college attendance"</span><span class="w">
  </span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_bw</span><span class="p">()</span><span class="w">
</span><span class="n">bayes_g</span><span class="w">
</span></code></pre></div></div>

<p><img src="../figures/multi_reg_predict_fig-1.png" width="100%" /></p>

<h3 id="compare-to-frequentist-analysis">Compare to frequentist analysis</h3>

<p>Inevitably, we (or a reviewer!) will want a comparison with a frequentist model.
I will tell you know that this is a fool’s errand full of philosophical,
statistical, and computational pitfalls and incommensurabilities. But
nonetheless, let’s see how this sort of analysis might look in a frequentist
model (as close as we can approximate anyway).</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## -----------------------------------------------------------------------------</span><span class="w">
</span><span class="c1">## quick comparison to frequentist approach</span><span class="w">
</span><span class="c1">## -----------------------------------------------------------------------------</span><span class="w">

</span><span class="c1">## fit logit model</span><span class="w">
</span><span class="n">lm_fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">glm</span><span class="p">(</span><span class="n">cbind</span><span class="p">(</span><span class="n">college</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">college</span><span class="p">)</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">raceeth</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">gender</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">pov185</span><span class="p">,</span><span class="w">
              </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df_tmp</span><span class="p">,</span><span class="w">
              </span><span class="n">family</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">binomial</span><span class="p">(</span><span class="s2">"logit"</span><span class="p">))</span><span class="w">

</span><span class="c1">## show summary</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="n">lm_fit</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## 
## Call:
## glm(formula = cbind(college, n - college) ~ raceeth * gender * 
##     pov185, family = binomial("logit"), data = df_tmp)
## 
## Coefficients: (2 not defined because of singularities)
##                                           Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)                              2.079e+00  7.500e-01   2.773  0.00556
## raceethasian                             1.048e+00  7.923e-01   1.323  0.18598
## raceethblackaa                          -2.652e-01  7.674e-01  -0.346  0.72965
## raceethhispnr                           -9.808e-01  1.377e+00  -0.712  0.47625
## raceethhisprs                           -1.215e-01  7.628e-01  -0.159  0.87345
## raceethmoretor                          -1.515e-01  7.657e-01  -0.198  0.84311
## raceethnhpi                              6.931e-01  1.275e+00   0.544  0.58661
## raceethunknown                           2.182e+01  6.648e+04   0.000  0.99974
## raceethwhite                             3.918e-02  7.524e-01   0.052  0.95847
## gendermale                              -1.192e+00  8.742e-01  -1.364  0.17265
## pov185below                             -2.166e+00  8.583e-01  -2.524  0.01160
## pov185unknown                           -1.520e+00  9.774e-01  -1.555  0.11996
## raceethasian:gendermale                  8.133e-01  9.381e-01   0.867  0.38596
## raceethblackaa:gendermale                8.365e-01  9.006e-01   0.929  0.35299
## raceethhispnr:gendermale                 1.480e+00  1.650e+00   0.897  0.36979
## raceethhisprs:gendermale                 4.404e-01  8.932e-01   0.493  0.62200
## raceethmoretor:gendermale                7.120e-01  8.980e-01   0.793  0.42785
## raceethnhpi:gendermale                   2.654e-01  1.487e+00   0.178  0.85841
## raceethunknown:gendermale                7.526e-01  8.025e-01   0.938  0.34831
## raceethwhite:gendermale                  5.916e-01  8.775e-01   0.674  0.50022
## raceethasian:pov185below                 1.409e+00  9.413e-01   1.496  0.13453
## raceethblackaa:pov185below               1.123e+00  8.815e-01   1.274  0.20261
## raceethhispnr:pov185below                1.132e+00  1.483e+00   0.764  0.44512
## raceethhisprs:pov185below                9.822e-01  8.745e-01   1.123  0.26142
## raceethmoretor:pov185below               1.122e+00  8.851e-01   1.268  0.20490
## raceethnhpi:pov185below                 -1.453e+00  1.508e+00  -0.964  0.33528
## raceethunknown:pov185below              -4.614e+01  1.036e+05   0.000  0.99964
## raceethwhite:pov185below                 6.669e-01  8.632e-01   0.773  0.43980
## raceethasian:pov185unknown               7.653e-03  1.027e+00   0.007  0.99405
## raceethblackaa:pov185unknown             2.307e-01  9.991e-01   0.231  0.81735
## raceethhispnr:pov185unknown              1.250e+00  1.528e+00   0.818  0.41338
## raceethhisprs:pov185unknown              1.244e-01  9.953e-01   0.125  0.90053
## raceethmoretor:pov185unknown             4.557e-01  1.006e+00   0.453  0.65060
## raceethnhpi:pov185unknown               -7.419e-01  1.597e+00  -0.465  0.64228
## raceethunknown:pov185unknown            -2.125e+01  6.648e+04   0.000  0.99974
## raceethwhite:pov185unknown               1.104e-01  9.823e-01   0.112  0.91055
## gendermale:pov185below                   1.279e+00  1.058e+00   1.209  0.22683
## gendermale:pov185unknown                 1.348e-02  1.173e+00   0.011  0.99083
## raceethasian:gendermale:pov185below     -1.368e+00  1.174e+00  -1.165  0.24391
## raceethblackaa:gendermale:pov185below   -1.401e+00  1.093e+00  -1.281  0.20012
## raceethhispnr:gendermale:pov185below    -1.161e+00  1.821e+00  -0.638  0.52363
## raceethhisprs:gendermale:pov185below    -9.009e-01  1.082e+00  -0.832  0.40522
## raceethmoretor:gendermale:pov185below   -1.334e+00  1.100e+00  -1.213  0.22496
## raceethnhpi:gendermale:pov185below       4.949e-01  1.856e+00   0.267  0.78974
## raceethunknown:gendermale:pov185below           NA         NA      NA       NA
## raceethwhite:gendermale:pov185below     -1.245e+00  1.066e+00  -1.168  0.24290
## raceethasian:gendermale:pov185unknown    4.412e-02  1.246e+00   0.035  0.97175
## raceethblackaa:gendermale:pov185unknown -5.135e-03  1.207e+00  -0.004  0.99661
## raceethhispnr:gendermale:pov185unknown  -1.221e+00  1.851e+00  -0.659  0.50959
## raceethhisprs:gendermale:pov185unknown   3.401e-01  1.201e+00   0.283  0.77710
## raceethmoretor:gendermale:pov185unknown  2.999e-03  1.216e+00   0.002  0.99803
## raceethnhpi:gendermale:pov185unknown    -2.907e-01  2.204e+00  -0.132  0.89508
## raceethunknown:gendermale:pov185unknown         NA         NA      NA       NA
## raceethwhite:gendermale:pov185unknown    1.494e-01  1.181e+00   0.127  0.89931
##                                           
## (Intercept)                             **
## raceethasian                              
## raceethblackaa                            
## raceethhispnr                             
## raceethhisprs                             
## raceethmoretor                            
## raceethnhpi                               
## raceethunknown                            
## raceethwhite                              
## gendermale                                
## pov185below                             * 
## pov185unknown                             
## raceethasian:gendermale                   
## raceethblackaa:gendermale                 
## raceethhispnr:gendermale                  
## raceethhisprs:gendermale                  
## raceethmoretor:gendermale                 
## raceethnhpi:gendermale                    
## raceethunknown:gendermale                 
## raceethwhite:gendermale                   
## raceethasian:pov185below                  
## raceethblackaa:pov185below                
## raceethhispnr:pov185below                 
## raceethhisprs:pov185below                 
## raceethmoretor:pov185below                
## raceethnhpi:pov185below                   
## raceethunknown:pov185below                
## raceethwhite:pov185below                  
## raceethasian:pov185unknown                
## raceethblackaa:pov185unknown              
## raceethhispnr:pov185unknown               
## raceethhisprs:pov185unknown               
## raceethmoretor:pov185unknown              
## raceethnhpi:pov185unknown                 
## raceethunknown:pov185unknown              
## raceethwhite:pov185unknown                
## gendermale:pov185below                    
## gendermale:pov185unknown                  
## raceethasian:gendermale:pov185below       
## raceethblackaa:gendermale:pov185below     
## raceethhispnr:gendermale:pov185below      
## raceethhisprs:gendermale:pov185below      
## raceethmoretor:gendermale:pov185below     
## raceethnhpi:gendermale:pov185below        
## raceethunknown:gendermale:pov185below     
## raceethwhite:gendermale:pov185below       
## raceethasian:gendermale:pov185unknown     
## raceethblackaa:gendermale:pov185unknown   
## raceethhispnr:gendermale:pov185unknown    
## raceethhisprs:gendermale:pov185unknown    
## raceethmoretor:gendermale:pov185unknown   
## raceethnhpi:gendermale:pov185unknown      
## raceethunknown:gendermale:pov185unknown   
## raceethwhite:gendermale:pov185unknown     
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1.6327e+03  on 51  degrees of freedom
## Residual deviance: 2.8359e-10  on  0  degrees of freedom
## AIC: 350.1
## 
## Number of Fisher Scoring iterations: 22
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## generage response predictions (meaning transform to probability scale)</span><span class="w">
</span><span class="n">lm_pred</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">lm_fit</span><span class="p">,</span><span class="w">
                   </span><span class="n">newdata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df_design</span><span class="p">,</span><span class="w">
                   </span><span class="n">se.fit</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w">
                   </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"response"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Warning in predict.lm(object, newdata, se.fit, scale = residual.scale, type =
## if (type == : prediction from rank-deficient fit; attr(*, "non-estim") has
## doubtful cases
</code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## wrangle data and join plot_index from Bayes plot so everything aligns; plot</span><span class="w">
</span><span class="c1">## means and 95 CIs to match prior plot</span><span class="w">
</span><span class="n">freq_g</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tibble</span><span class="p">(</span><span class="n">group</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">df_design</span><span class="o">$</span><span class="n">group</span><span class="p">,</span><span class="w">
                 </span><span class="n">pred</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lm_pred</span><span class="o">$</span><span class="n">fit</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w">
                 </span><span class="n">se</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lm_pred</span><span class="o">$</span><span class="n">se.fit</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">100</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w">
  </span><span class="n">mutate</span><span class="p">(</span><span class="n">ci95lo</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pred</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">se</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">qnorm</span><span class="p">(</span><span class="m">0.025</span><span class="p">),</span><span class="w">
         </span><span class="n">ci95hi</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pred</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">se</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">qnorm</span><span class="p">(</span><span class="m">0.975</span><span class="p">))</span><span class="w"> </span><span class="o">|&gt;</span><span class="w">
  </span><span class="n">left_join</span><span class="p">(</span><span class="n">pp_mean</span><span class="p">,</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"group"</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w">
  </span><span class="n">ggplot</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">plot_index</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pred</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_linerange</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">xmin</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ci95lo</span><span class="p">,</span><span class="w"> </span><span class="n">xmax</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ci95hi</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_point</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pred</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_x_continuous</span><span class="p">(</span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="p">),</span><span class="w">
                     </span><span class="n">minor_breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">annotate</span><span class="p">(</span><span class="s2">"rect"</span><span class="p">,</span><span class="w"> </span><span class="n">xmin</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="kc">Inf</span><span class="p">,</span><span class="w"> </span><span class="n">xmax</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">ymin</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">ymax</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">Inf</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.1</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">annotate</span><span class="p">(</span><span class="s2">"rect"</span><span class="p">,</span><span class="w"> </span><span class="n">xmin</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">xmax</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">Inf</span><span class="p">,</span><span class="w"> </span><span class="n">ymin</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">ymax</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">Inf</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.1</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="w">
    </span><span class="n">title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Frequentist predictions of college enrollment"</span><span class="p">,</span><span class="w">
    </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Group: race/ethnicity + gender + poverty status (185%)"</span><span class="p">,</span><span class="w">
    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Rate of college attendance"</span><span class="w">
  </span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_bw</span><span class="p">()</span><span class="w">

</span><span class="c1">## use patchwork to compare figures</span><span class="w">
</span><span class="n">freq_g</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">bayes_g</span><span class="w"> </span><span class="o">&amp;</span><span class="w">
  </span><span class="n">theme_bw</span><span class="p">(</span><span class="n">base_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">6</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="../figures/compare_fig-1.png" width="100%" /></p>

<p>With this quick comparison, we can see a number of weaknesses of the frequentist
approach:</p>

<ul>
  <li>95% confidence intervals are wider than Bayesian 95% credible intervals</li>
  <li>some 95% CIs cross 0 and 100%</li>
  <li>some groups don’t have estimates at all</li>
</ul>

<p>Of course, a frequentist might (rightfully!) counter:</p>

<ul>
  <li>we didn’t run a frequentist HLM</li>
  <li>our Bayesian estimates, especially for small groups, may be biased</li>
</ul>

<p>Unfortunately, there isn’t a single determining factor that will tell us that
one approach is better. That said, if one wants to provide estimates (with
estimates of error) for small groups, a Bayesian approach provides options that
a frequentist does not.</p>

<h2 id="comparing-groups">Comparing groups</h2>

<p>Often in the course of analyses that investigates heterogeneity, we want
estimates in the difference in effect between groups. In a frequentist analysis,
we’ll often look to see if parameters are statistically different from one
another (for example, their CIs don’t overlap) and then report the difference
between the estimates. However, this is a bit of a statistical fudge because our
estimates of error aren’t directly related to that difference. We can say pretty
confidently that they <em>are</em> different, but with less confidence about the <em>size</em>
of that difference.</p>

<p>With Bayesian posteriors, we can simply subtract one from the other to create a
posterior distribution of the difference.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## -----------------------------------------------------------------------------</span><span class="w">
</span><span class="c1">## comparison</span><span class="w">
</span><span class="c1">## -----------------------------------------------------------------------------</span><span class="w">

</span><span class="c1">## filter to two group comparison</span><span class="w">
</span><span class="n">pp_comp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">pp</span><span class="w"> </span><span class="o">|&gt;</span><span class="w">
  </span><span class="n">filter</span><span class="p">(</span><span class="n">group</span><span class="w"> </span><span class="o">%in%</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"white_male_above"</span><span class="p">,</span><span class="w"> </span><span class="s2">"white_male_below"</span><span class="p">))</span><span class="w">
</span></code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## plot two groups to make comparison clearer</span><span class="w">
</span><span class="n">comp_g</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">pp_comp</span><span class="w"> </span><span class="o">|&gt;</span><span class="w">
  </span><span class="n">ggplot</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">group</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">.epred</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">stat_pointinterval</span><span class="p">(</span><span class="n">.width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.95</span><span class="p">,</span><span class="w"> </span><span class="n">linewidth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.7</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_x_continuous</span><span class="p">(</span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">),</span><span class="w">
                     </span><span class="n">minor_breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">labs</span><span class="p">(</span><span class="w">
    </span><span class="n">title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Posterior predictive distributions of college enrollment"</span><span class="p">,</span><span class="w">
    </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Group: race/ethnicity + gender + poverty status (185%)"</span><span class="p">,</span><span class="w">
    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Rate of college attendance"</span><span class="w">
  </span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_bw</span><span class="p">()</span><span class="w">
</span><span class="n">comp_g</span><span class="w">
</span></code></pre></div></div>

<p><img src="../figures/difference_fig-1.png" width="100%" /></p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## wrangle data to subtract one group of predicitions from the other to get</span><span class="w">
</span><span class="c1">## estimate of difference</span><span class="w">
</span><span class="n">pp_diff</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tibble</span><span class="p">(</span><span class="n">wma</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pp_comp</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">filter</span><span class="p">(</span><span class="n">group</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s2">"white_male_above"</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">pull</span><span class="p">(</span><span class="n">.epred</span><span class="p">),</span><span class="w">
                  </span><span class="n">wmb</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pp_comp</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">filter</span><span class="p">(</span><span class="n">group</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s2">"white_male_below"</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">pull</span><span class="p">(</span><span class="n">.epred</span><span class="p">),</span><span class="w">
                  </span><span class="n">diff</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">wma</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">wmb</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## plot density of difference</span><span class="w">
</span><span class="n">diff_g</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">pp_diff</span><span class="w"> </span><span class="o">|&gt;</span><span class="w">
  </span><span class="n">ggplot</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">diff</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_density</span><span class="p">()</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_vline</span><span class="p">(</span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pp_diff</span><span class="o">$</span><span class="n">diff</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">mean</span><span class="p">(),</span><span class="w"> </span><span class="n">linetype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dashed"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_x_continuous</span><span class="p">(</span><span class="n">breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w">
                     </span><span class="n">minor_breaks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="m">0.5</span><span class="p">))</span><span class="w"> </span><span class="o">+</span><span class="w">
   </span><span class="n">labs</span><span class="p">(</span><span class="w">
     </span><span class="n">title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Difference in attendance rates"</span><span class="p">,</span><span class="w">
     </span><span class="n">subtitle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"(White, male, above 185%) - (White, male, below 185%)"</span><span class="p">,</span><span class="w">
     </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Density"</span><span class="p">,</span><span class="w">
     </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Percentage point difference"</span><span class="w">
   </span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">theme_bw</span><span class="p">()</span><span class="w">
</span><span class="n">diff_g</span><span class="w">
</span></code></pre></div></div>

<p><img src="../figures/plot_difference-1.png" width="100%" /></p>

<p>We can see that the difference in enrollment rates among white men below the
185% poverty line are about 29 percentage points less likely to enroll in
college than their white male peers above the poverty line. With this approach,
we can put a range on this estimate of about 25 to 33 percentage points.</p>




	<!-- <p> -->
	<!--   Updated: 18 November 2023 -->
	<!-- </p> -->
      </section>
      <footer>
        <p>
  Benjamin Skinner</br>  
  Assistant Professor</br>
  <a href="https://www.btskinner.io" class="iconlink" alt="Personal website">
    <i class="fas fa-home fa-lg"></i></a> | 
  <a href="https://github.com/btskinner" class="iconlink" alt="Github
								profile">
    <i class="fab fa-github fa-lg"></i></a> |
  <a href="https://twitter.com/btskinner" class="iconlink" alt="Twitter profile">
    <i class="fab fa-twitter fa-lg"></i></a>
</p>
<p>
  <small>
    <a href="https://pages.github.com">GitHub Pages</a> |
    <a href="https://github.com/orderedlist/minimal">Theme</a> |
    <a href="/ashe_bayes/releases/">Releases</a>
  </small>
</p>

      </footer>
    </div>

    <!-- load Javascript if page requires it -->
    
    
    

  </body>
</html>
