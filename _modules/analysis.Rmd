---
layout: module
title: Analytic Examples
subtitle: |
  | _A Gentle Introduction to Bayesian Analysis with Applications to QuantCrit_
  | ASHE Workshop
date: 18 November 2023
author: | 
  | Alberto Guzman-Alvarez
  | Taylor Burtch 
  | Benjamin Skinner
order: 1
category: module
links:
  data: college.RDS
  script: analysis.R
  pdf: analysis.pdf
output:
  md_document:
    variant: gfm
    preserve_yaml: true
header-includes:
  - \usepackage{amsmath}  
---

In this part of the workshop, we'll work through a couple of examples of a
Bayesian analysis. The first will be a very simple regression and the second a
more complex regression with deep interactions. We'll also consider a few model
checks and ways of presenting the results.


```{r knitr, include = FALSE, purl = FALSE}
source('knit_setup.R')
```
```{r header, include = FALSE, purl = TRUE}
################################################################################
##
## [ PROJ ] A Gentle Introduction to Bayesian Analysis with Applications
##           to QuantCrit | ASHE Workshop
## [ FILE ] analysis.R 
## [ INIT ] 18 November 2023
## [ AUTH ] Alberto Guzman-Alvarez, Taylor Burtch, Benjamin Skinner
##
################################################################################

```

## Libraries

We'll load a few libraries for our analysis:

- _tidyverse_: useful for data wrangling
- _brms_: our main Bayesian regression tool
- _bayesplot_: support library for plotting Bayesian results
- _tidybayes_: support library for wrangling/plotting Bayesian results
- _patchwork_: combine plots
- _shinystan_: interactive inspection of Bayesian objects
- _parallel_: take advantage of multiple cores

If you don't have these libraries, you can install them quickly with the
following:

```r
install.packages(c("tidyverse", "brms", "bayesplot", "tidybayes", "patchwork", "shinystan", "parallel"), dependencies = TRUE)

```

```{r libraries}
## ---------------------------
## libraries
## ---------------------------

libs <- c("tidyverse", "brms", "bayesplot", "tidybayes", "patchwork","shinystan")
sapply(libs, require, character.only = TRUE)
```

## Settings

We have a couple of settings that will help us. First, we'll take advantage of
our computers' multiple cores with `options(mc.cores=parallel::detectCores())`.

Because the estimation of Bayesian models involve pseudorandom computational
processes, we'll set a seed so that we get the same results. 

```{r settings}
## ---------------------------
## settings
## ---------------------------

## set number of cores to use to speed things up
options(mc.cores=parallel::detectCores())

## set a seed so things stay the same
my_seed <- 20231118
```

## Data

The data we're using today is a simplified version of HSLS09. Observations with
missing data have been dropped. We're also not accounting for the weighting
scheme used by NCES.

```{r read_data}
## ---------------------------
## input
## ---------------------------

df <- readRDS("college.RDS")
```

```{r show}
## ---------------------------
## show data set
## ---------------------------

df
```

## Simple regression

As a first step, we'll fit a regression of college attendance on a vector of
ones (intercept only model). This will give us the average attendance across the
full sample, with measure of uncertainty in the spread of the posterior
distribution.

With Bayesian models, we have to be a little more particular with our
distributional assumptions. Since our outcomes fall into 0s and 1s, we should
use a [Bernoulli
distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution) with a logit
link function for our likelihood in the `family` argument:

$$
f(k;p) = p^k(1-p)^{1-k}\, \text{for}\, k \in {0,1}
$$

We know our _k_ values here (0 for no college attendance and 1 for attendance).
Our unknown is _p_, the probability of attendance. 

### Running the model

```{r simple_reg}
#| message: FALSE
## -----------------------------------------------------------------------------
## simple regression: intercept only (average college-going rate)
## -----------------------------------------------------------------------------

## likelihood of going to college
fit <- brm(college ~ 1,
           data = df,
           family = bernoulli("logit"),
           seed = my_seed)
```

You should notice a few things. First, your model has to be compiled to a faster
coding language. For short models, it can take longer to compile than to run,
but for longer models, this isn't the case. You can also save the models so that
you don't have recompile each time.

Once it starts to run, you'll see a lot of information about leapfrog steps,
chains, warm-ups, and samples. We'll discuss some of these pieces and what they
mean in just a bit, but first we'll take a look at the summary statistics.

```{r summary_simple_reg}
#| message: FALSE
#| warning: FALSE

## show summary stats
summary(fit)
```

The intercept value is approximately 1.12. Remembering that this is on the logit
scale, we can covert this using an inverse logit transformation:

**Logit**

$$
ln(\frac{p}{1-p})
$$

**Inverse logit**

$$
\frac{1}{1 + e^{-x}}
$$

```{r inv_logit}
## helper function: inverse logit
inv_logit <- function(x) { 1 / (1 + exp(-x)) }

## convert intercept value to probability scale
inv_logit(1.12)
```

Approximately 75% of the sample attended college at some point. We also have an
estimated error of approximately 0.02 (back on the log scale). So far, this
seems much like you may be used seeing after fitting frequentist models. But
what is `Rhat`, `Bulk_ESS`, and `Tail_ESS`? Further, what is the meaning of all
the information in the `Draws`?

### A quick note on how we actually estimate the posterior

In applied Bayesian work, the posterior distribution, which is proportional to
the prior multiplied by the likelihood distributions, often doesn't have a
**closed form solution**. This just means we can't solve for the posterior using
analytic methods (e.g., algebraic equations). Instead, we are forced to sample
from the posterior and build an empirical distribution of our best guesses. This
is easier said than done because, remember, we don't know the answer.

Imagine I put you in a dark room and told you that I wanted you to be able to
describe the space to me. I assume the floor generally slopes to a low point, so
you'll probably make your way there. That said, the room may have dips in the
floor or weird corners. It also may be the size of a typical classroom or the
size of a football field. And also, I don't have infinite time or money for your
to spend your time figuring it out. How do you explore the space in way that's
complete but also efficient?

This is basically the problem facing applied Bayesian work. With the advent of
modern computing, we have a number of approaches. We unfortunately don't have
time to get into the nuances, benefits, and drawbacks of various samplers. The
main thing to know is that most make use of Markov chain Monte Carlo (MCMC)
chains and, as a result, return multiple "guesses" of our unknown parameters.
When we see the `Estimate` and `Est.Error` values in the results, these are
summary statistics of a full distribution of results.

_Why multiple chains?_: Let's go back to the room. If you only have so much time
to explore the room, where you start may influence where you explore. You may
not find the low point in the floor or you may get stuck in a corner. How do I
know you are giving me an accurate account of the room? But if you can start
multiple times in multiple places and end with similar results, that feels like
evidence you are describing the space, or at least the main features. This is
the logic behind running multiple chains.

Let's look at beginning of the chains with a **trace plot**, the first 50
samples:

```{r trace_simple_reg_1}
#| message: FALSE
#| warning: FALSE

## show trace of chains for intercept (our main parameter)
color_scheme_set("mix-blue-pink")
mcmc_trace(fit |> as_draws(inc_warmup = TRUE),
           pars = "b_Intercept", n_warmup = 1000,
           window = c(0, 50)) +
  labs(
    title = "Trace of posterior chains",
    subtitle = "Draws: 0 to 50"
  )
```

Notice how the four chains start in very different places. But very quickly for
this simple model they collectively **converge** to a similar area. Let's look
at the rest of the samples:


```{r trace_simple_reg_2}
#| message: FALSE
#| warning: FALSE

## show trace of chains for intercept (our main parameter)
color_scheme_set("mix-blue-pink")
mcmc_trace(fit |> as_draws(inc_warmup = TRUE),
           pars = "b_Intercept", n_warmup = 1000,
           window = c(500, 2000)) +
  labs(
    title = "Trace of posterior chains",
    subtitle = "Draws: 500 to 2000"
  )
```

Because we purposefully start the chains in different spots, we shouldn't think
those early samples represent the posterior. We throw these out as **warmup**
and only keep the later samples which are combined into our **total post-warmup
draws**. In this model, we end up with 4,000 sample draws of our posterior.

Seeing the consistent overlap or **mixing** of the chains, we feel we have a
well-performing model (at least in terms of computation). The `Rhat` statistic
gives us a more formal test of the ratio of between chain variance to within
chain variance. `Rhat` measures below 1.1 suggest we have good mixture. The two
ESS measures stand for **Effective Sample Size**. Due to the deterministic
nature of our sampler (and most samplers), the draws in our sample distribution
aren't entirely independent. `Bulk_ESS` sample size gives us the overall
effective sample size while `Tail_ESS` gives an estimate of effective sample
size in the tails of the distribution.

### Plotting the posterior

Since our Bayesian posterior represents a distribution of samples, let's look at
it graphically.

```{r plot_simple_reg_logit}
#| message: FALSE
#| warning: FALSE

## show distribution of intercept (our main parameter)
mcmc_areas(fit, prob = 0.95, pars = "b_Intercept") +
  labs(
    title = "Posterior distribution (log scale)",
    subtitle = "with median and 95% interval"
  )
```

As a quick trick, we'll use the `transformation` argument and our `inv_logit()`
function to see the posterior on the probability scale.

```{r plot_simple_reg_prob}
#| message: FALSE
#| warning: FALSE

## show distribution of transformed intercept (our main parameter)
## using helper function
mcmc_areas(fit, prob = 0.95, pars = "b_Intercept",
           transformation = list("b_Intercept" = inv_logit)) +
  labs(
    title = "Posterior distribution (probability)",
    subtitle = "with median and 95% interval; prior: student_t(0,3,2.5)"
  )
```

### Priors

What about priors? Don't we have to set those? By default, `brm()` will choose a
set of **weakly informative** priors based on the likelihood distribution you
set. You can check this after the fact using `prior_summary()`.

```{r check_prior}
## ---------------------------
## check prior
## ---------------------------

## show prior from first model
prior_summary(fit)
```

Let's change our prior to normal distribution with a wide variance (remember:
we're on a logit scale).


```{r change_prior}
## change to normal prior for comparison
fit <- brm(college ~ 1,
           data = df,
           family = bernoulli("logit"),
           seed = my_seed,
           prior = set_prior("normal(0,20)", class = "Intercept"))

## check prior
prior_summary(fit)

## show summary stats
summary(fit)

## show distribution of transformed intercept (our main parameter)
## using helper function
mcmc_areas(fit, prob = 0.95, pars = "b_Intercept",
           transformation = list("b_Intercept" = inv_logit)) +
  labs(
    title = "Posterior distribution (probability)",
    subtitle = "with median and 95% interval; prior: normal(0,20)",
  )
```

The posterior is largely the same. It may have taken slightly longer or shorter
to run the sampler, but again, for this model it's almost imperceptible. For
more complex models, however, prior selection can be the difference between a
model that converges and one that doesn't. You can also be more specific with
your priors. But for now, we'll keep using the defaults.

### Posterior check

Bayesians often talk about accurately modeling the **data generating process**.
You can think about your model being a machine that, if accurately built and
tuned, can be reversed to make predictions (new data) that look like what you
observed. If posterior predictions look quite different from what you observed
in your data, that's evidence you don't have a good model of the world that
produced your data.

We can check this by producing a number of predictions from our fitted model
using `posterior_epred()` and comparing it to our empirical mean of college
enrollment.

```{r pp_check}
## ---------------------------
## posterior predictive check
## ---------------------------

## plot our posterior predictive values against the college-going rate
## that is observed in the data
ppc_stat(y = df |> pull(college) |> c(),
         yrep = posterior_epred(fit),
         stat = mean)
```

Seeing that the distribution of our predictions $y^{rep}$ splits $y$, we have
evidence that our machine/model is well specified.


## Speed up trick

An unfortunate thing about Bayesian samplers is that they don't scale well with
data size. In some cases --- such as our own --- we can exploit connections
between distributions and sufficient statistics to make our data matrix smaller
and therefore sampler faster.

A Bernoulli distribution (coin flip) can be thought of a binomial distribution
(successes out of trials) in which there is only one trial. By collapsing our
data to a single row of the number of students who went to college out of the
total, we can rewrite our model and speed it up.

```{r speed_up}
## -----------------------------------------------------------------------------
## speed up trick
## -----------------------------------------------------------------------------

## since we're using categorical variables (rather than continuous variables),
## collapse binary data (bernoulli) into smaller data set of successes/trials
## (binomial) to take advantage of sufficient statistics
df_tmp <- df |>
  summarise(college = sum(college),
            n = n())

## likelihood of going to college using binomial
fit <- brm(college | trials(n) ~ 1,
           data = df_tmp,
           family = binomial("logit"),
           seed = my_seed)

## show summary stats
summary(fit)
```

As we can see, we get the same results. We'll use this speed up trick with our
more complex model below.

## Multiple regression model with deep interactions

In this model, we'll interact our categorical indicators for race/ethnicity,
gender, and poverty status. We'll include main effects for each characteristic
individually, but also random intercept adjustments for interactions for each
group. These random effects will be especially important for providing estimates
for small groups.


```{r multi_reg}
## -----------------------------------------------------------------------------
## multiple regression across groups
## -----------------------------------------------------------------------------

## collapse into groups of race/ethnicity by gender by poverty level
df_tmp <- df |>
  group_by(raceeth, gender, pov185) |>
  summarise(college = sum(college),
            n = n(),
            .groups = "drop")

## likelihood of going to college using binomial
fit <- brm(college | trials(n) ~ raceeth + gender + pov185 +
             (1 | raceeth:gender:pov185),
           data = df_tmp,
           family = binomial("logit"),
           seed = my_seed)

## show summary stats
summary(fit)
```

### Shiny Stan

A wonderful tool for inspecting our model fit is Shiny Stan. It performs many of
the checks we performed before plus many more in an interactive browser window.
To open the program, type the following on the console line:

```r
launch_shinystan(fit)
```

### Predictions

To make these marginal effect estimates easier to interpret, we'll produce
posterior predictions again. One thing that's very easy to do with a model like
this is make predictions even for groups that aren't observed in the data. To do
that, we will make a design matrix that includes all possible combinations of
the demographic characteristics we modeled.

After that, we'll produce a plot showing the posterior predictions for each
group.


```{r multi_reg_predict}
## ---------------------------
## posterior predictions
## ---------------------------

## create a design matrix (data frame) of all possible groups in our model
df_design <- expand.grid(raceeth = df |> distinct(raceeth) |> pull() |> c(),
                         gender = df |> distinct(gender) |> pull() |> c(),
                         pov185 = df |> distinct(pov185) |> pull() |> c(),
                         stringsAsFactors = FALSE) |>
  as_tibble() |>
  arrange(raceeth, gender, pov185) |>
  mutate(n = 100,
         group = paste(raceeth, gender, pov185, sep = "_"))

## get posterior predictions but in long form that's better for plotting
pp <- df_design |>
  add_epred_draws(fit,
                  ndraws = 500,
                  allow_new_levels = TRUE)

## compute mean posterior by group to get order for plot
pp_mean <- pp |>
  summarise(pp_mean = mean(.epred),
            .groups = "drop") |>
  arrange(pp_mean) |>
  mutate(plot_index = row_number(),
         plot_index = factor(plot_index,
                             levels = plot_index,
                             labels = group)) |>
  select(group, pp_mean, plot_index)
```

```{r multi_reg_predict_fig}
## join means back to main pp tibble and plot densities for each group
bayes_g <- pp |>
  left_join(pp_mean, by = "group") |>
  ggplot(aes(y = plot_index, x = .epred)) +
  stat_pointinterval(.width = 0.95, linewidth = 0.7, size = 1) +
  scale_x_continuous(breaks = seq(0, 100, 10),
                     minor_breaks = seq(0, 100, 5)) +
  labs(
    title = "Posterior predictive distributions of college enrollment",
    y = "Group: race/ethnicity + gender + poverty status (185%)",
    x = "Rate of college attendance"
  ) +
  theme_bw()
bayes_g
```

### Compare to frequentist analysis

Inevitably, we (or a reviewer!) will want a comparison with a frequentist model.
I will tell you know that this is a fool's errand full of philosophical,
statistical, and computational pitfalls and incommensurabilities. But
nonetheless, let's see how this sort of analysis might look in a frequentist
model (as close as we can approximate anyway).

```{r compare}
## -----------------------------------------------------------------------------
## quick comparison to frequentist approach
## -----------------------------------------------------------------------------

## fit logit model
lm_fit <- glm(cbind(college, n - college) ~ raceeth * gender * pov185,
              data = df_tmp,
              family = binomial("logit"))

## show summary
summary(lm_fit)

## generage response predictions (meaning transform to probability scale)
lm_pred <- predict(lm_fit,
                   newdata = df_design,
                   se.fit = TRUE,
                   type = "response")
```

```{r compare_fig}
## wrangle data and join plot_index from Bayes plot so everything aligns; plot
## means and 95 CIs to match prior plot
freq_g <- tibble(group = df_design$group,
                 pred = lm_pred$fit * 100,
                 se = lm_pred$se.fit * 100) |>
  mutate(ci95lo = pred + se * qnorm(0.025),
         ci95hi = pred + se * qnorm(0.975)) |>
  left_join(pp_mean, by = "group") |>
  ggplot(aes(y = plot_index, x = pred)) +
  geom_linerange(aes(xmin = ci95lo, xmax = ci95hi)) +
  geom_point(aes(x = pred)) +
  scale_x_continuous(breaks = seq(0, 100, 10),
                     minor_breaks = seq(0, 100, 5)) +
  annotate("rect", xmin = -Inf, xmax = 0, ymin = 0, ymax = Inf, alpha = 0.1) +
  annotate("rect", xmin = 100, xmax = Inf, ymin = 0, ymax = Inf, alpha = 0.1) +
  labs(
    title = "Frequentist predictions of college enrollment",
    y = "Group: race/ethnicity + gender + poverty status (185%)",
    x = "Rate of college attendance"
  ) +
  theme_bw()

## use patchwork to compare figures
freq_g / bayes_g &
  theme_bw(base_size = 6)
```

With this quick comparison, we can see a number of weaknesses of the frequentist
approach:

- 95% confidence intervals are wider than Bayesian 95% credible intervals
- some 95% CIs cross 0 and 100%
- some groups don't have estimates at all

Of course, a frequentist might (rightfully!) counter:

- we didn't run a frequentist HLM
- our Bayesian estimates, especially for small groups, may be biased

Unfortunately, there isn't a single determining factor that will tell us that
one approach is better. That said, if one wants to provide estimates (with
estimates of error) for small groups, a Bayesian approach provides options that
a frequentist does not.

## Comparing groups

Often in the course of analyses that investigates heterogeneity, we want
estimates in the difference in effect between groups. In a frequentist analysis,
we'll often look to see if parameters are statistically different from one
another (for example, their CIs don't overlap) and then report the difference
between the estimates. However, this is a bit of a statistical fudge because our
estimates of error aren't directly related to that difference. We can say pretty
confidently that they _are_ different, but with less confidence about the _size_
of that difference.

With Bayesian posteriors, we can simply subtract one from the other to create a
posterior distribution of the difference. 

```{r difference}
## -----------------------------------------------------------------------------
## comparison
## -----------------------------------------------------------------------------

## filter to two group comparison
pp_comp <- pp |>
  filter(group %in% c("white_male_above", "white_male_below"))
```

```{r difference_fig}
## plot two groups to make comparison clearer
comp_g <- pp_comp |>
  ggplot(aes(y = group, x = .epred)) +
  stat_pointinterval(.width = 0.95, linewidth = 0.7, size = 1) +
  scale_x_continuous(breaks = seq(0, 100, 5),
                     minor_breaks = seq(0, 100, 1)) +
  labs(
    title = "Posterior predictive distributions of college enrollment",
    y = "Group: race/ethnicity + gender + poverty status (185%)",
    x = "Rate of college attendance"
  ) +
  theme_bw()
comp_g
```
```{r compute_diff}
## wrangle data to subtract one group of predicitions from the other to get
## estimate of difference
pp_diff <- tibble(wma = pp_comp |> filter(group == "white_male_above") |> pull(.epred),
                  wmb = pp_comp |> filter(group == "white_male_below") |> pull(.epred),
                  diff = wma - wmb)
```

```{r plot_difference}
## plot density of difference
diff_g <- pp_diff |>
  ggplot(aes(x = diff)) +
  geom_density() +
  geom_vline(xintercept = pp_diff$diff |> mean(), linetype = "dashed") +
  scale_x_continuous(breaks = seq(0, 100, 1),
                     minor_breaks = seq(0, 100, 0.5)) +
   labs(
     title = "Difference in attendance rates",
     subtitle = "(White, male, above 185%) - (White, male, below 185%)",
     y = "Density",
     x = "Percentage point difference"
   ) +
  theme_bw()
diff_g
```

We can see that the difference in enrollment rates among white men below the
185% poverty line are about 29 percentage points less likely to enroll in
college than their white male peers above the poverty line. With this approach,
we can put a range on this estimate of about 25 to 33 percentage points.


```{r, include = FALSE, purl = TRUE}
## -----------------------------------------------------------------------------
## end script
## -----------------------------------------------------------------------------
```
